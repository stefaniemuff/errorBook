[
["intro.html", "Chapter 1 Introduction 1.1 What is Error? 1.2 Why and When do I Have to Worry? 1.3 Organization and Take-Home Messages of This Book", " Chapter 1 Introduction 1.1 What is Error? In our interactions with applied scientists, we often realize that ‘’error’’ is either regarded as something systematic, in the sense of a bias, potentially due to an inherent problem in the data collection process, or as the result of an erroneous step when data are handled and stored, like writing down a wrong number into the lab journal. However, the measurement error that we are talking about in this book is something much more universal. It should me understood as an uncertainty of measurements, which, so some degree, is present in virtually all data. To paraphrase Max Planck (in The Meaning and Limits of Exact Science, 1949): Measurement error is an uncertainty in our recording of Nature’s answer to our questions When measurement error is understood as the uncertainty that remains about an entity after a measurement of that entity was taken, it becomes obvious that measurement error may originate due to a wide variety of reasons, such as Measurement imprecision in the field or the lab, which may arise due to limited accuracy of an instrument, or because the targeted value is difficult to measure or volatile, for example blood pressure, body weight etc. Incomplete or biased observations, for example due to preferential sampling. Misclassification of categorical variables, e.g. when deciding if a disease is present or not. Rounding or digit preference. Temporal or spatial misalignments of observations, e.g. in interval samples GPS observations of telemetry studies. Measurement of an entity of interest by taking measurements on a substitute, e.g. Self-reporting of numbers, e.g. by patients in medicine or in food behaviour studies. This is of course by no means a comprehensive list, and we are sure many readers could immediately come up with more examples. We put on the record that measurement errors should not, in generaly, merely be seen as ‘’mistakes’’, but as uncertainty that is inherently present due to the limitations of our ability to collect information in the real world. It is surprising how many phenomena in statistics and its applications can be viewed through the measurement error lens. Prominant examples are the concept of heritability in genetics and evolutionary biology, as we will explain in Section 3.2, or the fact that omitting informative covariates in regression models is analogous to introducing a certain type of measurement error, the Berkson error, which we will introduce in Section 2.1.2. 1.2 Why and When do I Have to Worry? An important question, and one that is central to this book, concerns the effect measurement error in the data, given that the aim is to estimate the parameters of a model. Measurement error has essentialy three effects, which Carroll et al. (2006) denote as the Triple Whammy of Measurement Error: Parameter estimators of statistical models are biased. It leads to loss of statistical power to detect relationships. Features of the data are masked in graphical analyses. To be illustrated with simple and/or real examples for classical measurement error, with reference to Section 2.1.1. 1.2.1 When is Error a Problem? This is a question practicioners often ask. It is, however, impossible to give general guidelines about when measurement error is ‘’severe’’ enough to affect parameters estimates and conclusions drawn from an analysis in a way that cannot be tolerated. A rather universal advice we can give is to simulate data from the model at hand, and check what happens to the estimated parameters, standard errors and - more broadly - the conclusions that are drawn when error of the type that is expected is artificially generated in the error-prone variables. Of course, this already requires a rough understanding of the error model at hand, but without such a model it is anyway elusive to hypothesize about the effect of the error. Let us look at a very simple simulation setup for a linear regression model, with a covariate \\(x\\sim\\mathsf{N}(0,1)\\) and a response \\(y\\), generated according to the simple regression model \\[y = \\beta_0 + \\beta_x x + \\epsilon \\ , \\quad \\epsilon \\sim \\mathsf{N}(0,1) \\ ,\\] with \\(\\beta_0=\\beta_x=1\\). set.seed(1234) x &lt;- rnorm(100) y &lt;- 1 + x + rnorm(100) summary(lm(y~x))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0371541 0.1049788 9.879657 2.208449e-16 ## x 0.9739151 0.1037759 9.384789 2.620462e-15 Now let us add measurement error to the \\(x\\) covariate, namely by assuming we see a variable \\(w = x +u,\\) which is an error-prone version of \\(x\\) with measurement error \\(u\\sim\\mathsf{N}(0,\\sigma_u^2)\\) and measurement error variance \\(\\sigma_u^2=1\\). This is a so-called classical measurement error model (see Section 2.1.1), which is known to attenuate the estimated slope parameters, as can be verified by a simple simulation. Let us generate an error-prone version \\(w\\) and then regress \\(y\\) against \\(w\\): w &lt;- x + rnorm(100) summary(lm(y~w))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8856732 0.11826999 7.488571 3.076746e-11 ## w 0.5522517 0.08225006 6.714301 1.240119e-09 We see that that estimated slope for \\(w\\), \\(\\hat\\beta_w=0.55\\) is much smaller than the actual slope \\(\\hat\\beta_x=0.97\\) that was estimated in the regression of \\(y\\) against \\(x\\). This severe bias occurs because we have assumed that the error variance is as large as the actual variance of the true covariate (\\(\\sigma_u^2=\\sigma_x^2=1\\)), which may not necessarily by realistic in practical applications. In any case, by playing with the error variance \\(\\sigma_u^2\\), the user can generate a feeling for the effect of the error at hand. A simulation for this simple linear regression model with classical error has been implemented in a Shiny app (see 3.1). Note, however, that the situation in applications is typically quite a bit more complicated. When it comes to the decision whether one should worry or not, we strongly believe this depends on the context. Ideally, the user has pre-specified which effects can be tolerated before actually simulating the error, and it can then be decided whether the effect is negligible in that sense. Otherwise, the error must be accounted for by an appropriate error model. In that case, this book is for you! 1.2.2 Bias Versus Variance Todo: Look at bias vs variance part in Carroll book, but then point out that the (typically) larger variance is also the more honest estimate, because error obscures information, and by ignoring it we pretend to be more certain about a (potentially biased) estimate than we really are. 1.2.3 Is it sometimes better not to model the error? If error is unknown, correction can go in the wrong direction Therefore it is crucial to find an error model Plan right during data collection process 1.3 Organization and Take-Home Messages of This Book How the book is organized Examples and R code (how will it be made available?) What we are going to do, outlook to chapters. References "]
]
